{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from dataloaders.pmnist import get\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets,transforms\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "torch.random.manual_seed(0) \n",
    "import torch.nn.functional as F\n",
    "from networks.power_spherical import PowerSpherical\n",
    "from approaches.direction_ucl import Appr\n",
    "from networks.mlp_direction_ucl import Model\n",
    "from dataloaders.pmnist import get\n",
    "from torch.distributions import LogNormal, Normal, Bernoulli\n",
    "from copy import deepcopy \n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = LogNormal(0, 1)\n",
    "x = m.sample(torch.Size([10000])) + -2\n",
    "print(x.shape)\n",
    "a = plt.hist(x.cpu().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7077,  0.4038,  0.3765,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.8361,  0.3083,  0.0789,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.7968,  0.6082,  0.3411,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "weight = nn.Linear(3,3)(torch.rand(3,3))\n",
    "# print(weight.data)\n",
    "print(torch.cat([weight, torch.zeros(3,5)], dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3623],\n",
      "        [0.3521],\n",
      "        [0.3482],\n",
      "        [0.2130],\n",
      "        [0.1870],\n",
      "        [0.2717],\n",
      "        [0.3028],\n",
      "        [0.2843],\n",
      "        [0.2850],\n",
      "        [0.1708]], device='cuda:0')\n",
      "tensor([[0.0284],\n",
      "        [0.0264],\n",
      "        [0.0226],\n",
      "        [0.0218],\n",
      "        [0.0246],\n",
      "        [0.0237],\n",
      "        [0.0234],\n",
      "        [0.0221],\n",
      "        [0.0254],\n",
      "        [0.0217]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[0.3606],\n",
      "        [0.3653],\n",
      "        [0.3402],\n",
      "        [0.2014],\n",
      "        [0.1825],\n",
      "        [0.2638],\n",
      "        [0.3069],\n",
      "        [0.2868],\n",
      "        [0.2776],\n",
      "        [0.1767]], device='cuda:0', requires_grad=True)\n",
      "tensor([[0.0257],\n",
      "        [0.0238],\n",
      "        [0.0214],\n",
      "        [0.0204],\n",
      "        [0.0229],\n",
      "        [0.0208],\n",
      "        [0.0215],\n",
      "        [0.0216],\n",
      "        [0.0243],\n",
      "        [0.0191]], device='cuda:0', grad_fn=<SoftplusBackward>)\n"
     ]
    }
   ],
   "source": [
    "# model = Model(ratio=0.5, eps=0.1).cuda()\n",
    "# appr = Appr(model, model_name='mlp', data_name='pMNIST', lr=0.001, sbatch=256, optim='Adam')\n",
    "# model.load_state_dict(torch.load('./trained_model/{}_task_{}.model'.format(appr.file_name, 0)))\n",
    "# appr.model_old = deepcopy(appr.get_model(appr.model))\n",
    "# model.load_state_dict(torch.load('./trained_model/{}_task_{}.model'.format(appr.file_name, 1)))\n",
    "layer = model.fc3\n",
    "saver_layer = appr.model_old['fc3']\n",
    "mu = layer.rad_mu\n",
    "sigma = F.softplus(layer.rad_rho)\n",
    "q = LogNormal(mu, sigma)\n",
    "# print(q.mean)\n",
    "print(saver_layer['rad_mu'])\n",
    "print(F.softplus(saver_layer['rad_rho']))\n",
    "print(mu)\n",
    "print(sigma)\n",
    "# print(q.variance)\n",
    "# print(mu - sigma ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get('MNIST')\n",
    "xtrain = [data[t]['train']['x'] for t in data]\n",
    "ytrain = [data[t]['train']['y'] for t in data]\n",
    "xtrain = torch.cat(xtrain)\n",
    "ytrain = torch.cat(ytrain)\n",
    "\n",
    "xvalid = [data[t]['valid']['x'] for t in data]\n",
    "yvalid = [data[t]['valid']['y'] for t in data]\n",
    "xvalid = torch.cat(xvalid)\n",
    "yvalid = torch.cat(yvalid)\n",
    "\n",
    "# model = Model(3, 200)\n",
    "# appr = Appr(model, 'normal',is_bayesian=True, sbatch=128, lr=0.01)\n",
    "# with torch.autograd.set_detect_anomaly(True):\n",
    "#     appr.train(xtrain, ytrain, xvalid, yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_var = 2 / 400\n",
    "noise_var = total_var * 0.001\n",
    "mu_var = total_var - noise_var\n",
    "        \n",
    "noise_std, mu_std = math.sqrt(noise_var), math.sqrt(mu_var)\n",
    "bound = math.sqrt(3.0) * mu_std\n",
    "rho_init = np.log(np.exp(noise_std)-1)\n",
    "print(noise_std, bound)\n",
    "from torch.distributions import Normal\n",
    "print(Normal(0, noise_std).rsample())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concentration init 3699.4736842105276\n",
      "sigma init 0.03571428571428571 mu init 0.06185895741317418\n",
      "concentration init 1880.526315789474\n",
      "sigma init 0.05 mu init 0.08660254037844387\n",
      "concentration init 1880.526315789474\n",
      "sigma init 0.05 mu init 0.08660254037844387\n",
      "test acc: 0.9848\n",
      "| Epoch   1, time=2890.7ms/503.6ms | Train: loss=0.152, acc=95.43% | Valid: loss=0.165, acc=95.14% | *\n",
      "| Epoch   2, time=2829.1ms/535.8ms | Train: loss=0.104, acc=96.82% | Valid: loss=0.126, acc=95.89% | *\n",
      "| Epoch   3, time=2880.5ms/507.2ms | Train: loss=0.073, acc=97.64% | Valid: loss=0.104, acc=96.81% | *\n",
      "| Epoch   4, time=2790.6ms/493.5ms | Train: loss=0.050, acc=98.44% | Valid: loss=0.086, acc=97.45% | *\n",
      "| Epoch   5, time=2816.9ms/510.3ms | Train: loss=0.045, acc=98.59% | Valid: loss=0.082, acc=97.62% | *\n",
      "| Epoch   6, time=2859.2ms/500.4ms | Train: loss=0.043, acc=98.64% | Valid: loss=0.086, acc=97.50% |\n",
      "| Epoch   7, time=2960.0ms/530.5ms | Train: loss=0.036, acc=98.85% | Valid: loss=0.079, acc=97.66% | *\n",
      "| Epoch   8, time=2805.2ms/506.8ms | Train: loss=0.028, acc=99.15% | Valid: loss=0.071, acc=97.82% | *\n",
      "| Epoch   9, time=2899.1ms/498.6ms | Train: loss=0.021, acc=99.36% | Valid: loss=0.068, acc=98.06% | *\n",
      "| Epoch  10, time=2980.7ms/540.7ms | Train: loss=0.018, acc=99.44% | Valid: loss=0.068, acc=98.10% | *\n",
      "| Epoch  11, time=3051.0ms/555.0ms | Train: loss=0.022, acc=99.33% | Valid: loss=0.076, acc=97.76% |\n",
      "| Epoch  12, time=2914.0ms/531.1ms | Train: loss=0.016, acc=99.51% | Valid: loss=0.072, acc=97.96% |\n",
      "| Epoch  13, time=3439.8ms/533.5ms | Train: loss=0.018, acc=99.41% | Valid: loss=0.074, acc=98.08% |\n",
      "| Epoch  14, time=3157.1ms/497.4ms | Train: loss=0.019, acc=99.40% | Valid: loss=0.077, acc=97.75% |\n",
      "| Epoch  15, time=2851.8ms/506.7ms | Train: loss=0.020, acc=99.35% | Valid: loss=0.080, acc=97.91% | lr=3.3e-04\n",
      "| Epoch  16, time=2801.9ms/518.6ms | Train: loss=0.006, acc=99.86% | Valid: loss=0.064, acc=98.30% | *\n",
      "| Epoch  17, time=2763.6ms/527.1ms | Train: loss=0.004, acc=99.92% | Valid: loss=0.059, acc=98.51% | *\n",
      "| Epoch  18, time=2752.0ms/540.6ms | Train: loss=0.003, acc=99.95% | Valid: loss=0.060, acc=98.41% |\n",
      "| Epoch  19, time=2838.0ms/520.2ms | Train: loss=0.003, acc=99.93% | Valid: loss=0.062, acc=98.56% | *\n",
      "| Epoch  20, time=2798.3ms/499.2ms | Train: loss=0.003, acc=99.95% | Valid: loss=0.064, acc=98.51% |\n",
      "| Epoch  21, time=2783.6ms/527.6ms | Train: loss=0.002, acc=99.96% | Valid: loss=0.061, acc=98.42% |\n",
      "| Epoch  22, time=3069.9ms/637.2ms | Train: loss=0.002, acc=99.95% | Valid: loss=0.066, acc=98.47% |\n",
      "| Epoch  23, time=3043.1ms/518.2ms | Train: loss=0.002, acc=99.97% | Valid: loss=0.066, acc=98.37% |\n",
      "| Epoch  24, time=2856.5ms/497.4ms | Train: loss=0.002, acc=99.98% | Valid: loss=0.062, acc=98.55% | lr=1.1e-04\n",
      "| Epoch  25, time=2854.3ms/498.7ms | Train: loss=0.001, acc=99.98% | Valid: loss=0.062, acc=98.57% | *\n",
      "| Epoch  26, time=2757.3ms/502.8ms | Train: loss=0.001, acc=99.99% | Valid: loss=0.059, acc=98.59% | *\n",
      "| Epoch  27, time=2782.1ms/501.1ms | Train: loss=0.001, acc=99.99% | Valid: loss=0.063, acc=98.55% |\n",
      "| Epoch  28, time=2780.9ms/500.3ms | Train: loss=0.001, acc=99.99% | Valid: loss=0.063, acc=98.60% | *\n",
      "| Epoch  29, time=2855.0ms/511.3ms | Train: loss=0.001, acc=99.98% | Valid: loss=0.066, acc=98.54% |\n",
      "| Epoch  30, time=2772.1ms/506.2ms | Train: loss=0.001, acc=99.99% | Valid: loss=0.064, acc=98.59% |\n",
      "| Epoch  31, time=2751.3ms/545.6ms | Train: loss=0.001, acc=99.99% | Valid: loss=0.066, acc=98.59% |\n",
      "| Epoch  32, time=2823.9ms/532.3ms | Train: loss=0.001, acc=99.99% | Valid: loss=0.066, acc=98.62% | *\n",
      "| Epoch  33, time=2952.6ms/550.5ms | Train: loss=0.001, acc=99.99% | Valid: loss=0.066, acc=98.55% |\n",
      "| Epoch  34, time=2921.5ms/501.6ms | Train: loss=0.000, acc=99.99% | Valid: loss=0.064, acc=98.66% | *\n",
      "| Epoch  35, time=2859.0ms/525.7ms | Train: loss=0.000, acc=99.99% | Valid: loss=0.067, acc=98.63% |\n",
      "| Epoch  36, time=2900.1ms/537.7ms | Train: loss=0.001, acc=99.99% | Valid: loss=0.070, acc=98.56% |\n",
      "| Epoch  37, time=2810.1ms/497.4ms | Train: loss=0.000, acc=99.99% | Valid: loss=0.067, acc=98.56% |\n",
      "| Epoch  38, time=3200.3ms/507.6ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.067, acc=98.56% |\n",
      "| Epoch  39, time=2878.7ms/501.4ms | Train: loss=0.000, acc=99.99% | Valid: loss=0.068, acc=98.57% | lr=3.7e-05\n",
      "| Epoch  40, time=2967.1ms/510.9ms | Train: loss=0.000, acc=99.99% | Valid: loss=0.069, acc=98.63% |\n",
      "| Epoch  41, time=2847.8ms/527.0ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.069, acc=98.61% |\n",
      "| Epoch  42, time=2934.7ms/505.2ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.069, acc=98.55% |\n",
      "| Epoch  43, time=2977.4ms/520.4ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.069, acc=98.53% |\n",
      "| Epoch  44, time=3007.2ms/518.1ms | Train: loss=0.000, acc=99.99% | Valid: loss=0.068, acc=98.64% | lr=1.2e-05\n",
      "| Epoch  45, time=2965.9ms/541.7ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.068, acc=98.60% |\n",
      "| Epoch  46, time=2934.7ms/540.3ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.068, acc=98.62% |\n",
      "| Epoch  47, time=3040.3ms/520.6ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.068, acc=98.62% |\n",
      "| Epoch  48, time=2830.7ms/510.0ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.068, acc=98.63% |\n",
      "| Epoch  49, time=2922.0ms/517.7ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.069, acc=98.56% | lr=4.1e-06\n",
      "test acc: 0.6223 0.9863\n",
      "| Epoch   1, time=2865.0ms/529.1ms | Train: loss=0.144, acc=95.60% | Valid: loss=0.169, acc=95.05% | *\n",
      "| Epoch   2, time=2931.3ms/513.5ms | Train: loss=0.088, acc=97.31% | Valid: loss=0.121, acc=96.24% | *\n",
      "| Epoch   3, time=2965.3ms/529.0ms | Train: loss=0.066, acc=97.96% | Valid: loss=0.108, acc=96.86% | *\n",
      "| Epoch   4, time=2972.6ms/531.1ms | Train: loss=0.052, acc=98.38% | Valid: loss=0.096, acc=97.19% | *\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-eda8a8a1eb0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0my_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mappr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mtest_accs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python-projects\\Continual_Learning-main\\approaches\\direction_ucl.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, t, xtrain, ytrain, xvalid, yvalid)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mnum_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mclock1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python-projects\\Continual_Learning-main\\approaches\\direction_ucl.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mxent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# loss = self.custom_regularization(self.model_old, self.get_model(self.model), mini_batch_size, xent)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m             \u001b[0mkld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxent\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkld\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python-projects\\Continual_Learning-main\\approaches\\direction_ucl.py\u001b[0m in \u001b[0;36mkl_divergence\u001b[1;34m(self, saver_net, trainer_net)\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[0mkld_rad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_rad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_rad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m             \u001b[0mmu_bias_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer_bias\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msaver_bias\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msaver_rad_sigma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[0mkld\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mkld_dir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkld_rad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmu_bias_reg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdir_loc_reg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelevant_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_infos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1291\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1292\u001b[0m             \u001b[0m_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# noqa: C416 TODO: rewrite as list(range(m))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1293\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \u001b[1;31m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from approaches.direction_ucl import Appr\n",
    "from networks.mlp_direction_ucl import Model\n",
    "from dataloaders.pmnist import get\n",
    "from copy import deepcopy \n",
    "import torch\n",
    "\n",
    "# from networks.mlp import Model\n",
    "data = get(tasknum=10, seed=23081999)\n",
    "model = Model(ratio=0.5, eps=0.1).cuda()\n",
    "appr = Appr(model, model_name='mlp', data_name='pMNIST', lr=0.001, sbatch=256, optim='Adam')\n",
    "appr.model.load_state_dict(torch.load('./trained_model/{}_task_{}.model'.format(appr.file_name, 0)))\n",
    "appr.model_old = deepcopy(appr.get_model(appr.model))\n",
    "appr.saved = 1\n",
    "x_test = data[0]['test']['x'].cuda()\n",
    "y_test = data[0]['test']['y'].cuda()\n",
    "print('test acc:', appr.eval(x_test, y_test)[-1])\n",
    "for t in data.keys():\n",
    "    if t == 0: continue\n",
    "    x_train = data[t]['train']['x'].cuda()\n",
    "    y_train = data[t]['train']['y'].cuda()\n",
    "\n",
    "    x_valid = data[t]['valid']['x'].cuda()\n",
    "    y_valid = data[t]['valid']['y'].cuda()\n",
    "\n",
    "    appr.train(t, x_train, y_train, x_valid, y_valid)\n",
    "    test_accs = []\n",
    "    for i in range(t+1):\n",
    "        x_test = data[i]['test']['x'].cuda()\n",
    "        y_test = data[i]['test']['y'].cuda()\n",
    "        test_accs.append(str(appr.eval(x_test, y_test)[-1]))\n",
    "    \n",
    "    log_str = ' '.join(test_accs)\n",
    "    print('test acc:', log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from approaches.normal import Appr\n",
    "from networks.mlp import Model as Model\n",
    "from dataloaders.pmnist import get\n",
    "import torch\n",
    "from torch.nn.functional import softplus\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from networks.mlp import Model\n",
    "data = get(tasknum=10, seed=23081999)\n",
    "model = Model().cuda()\n",
    "appr = Appr(model, is_bayesian=False, lr=0.001, sbatch=256, optim='Adam')\n",
    "\n",
    "x_train = data[0]['train']['x'].cuda()\n",
    "y_train = data[0]['train']['y'].cuda()\n",
    "\n",
    "x_valid = data[0]['valid']['x'].cuda()\n",
    "y_valid = data[0]['valid']['y'].cuda()\n",
    "\n",
    "x_test = data[0]['test']['x'].cuda()\n",
    "y_test = data[0]['test']['y'].cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   1, time=595.8ms/166.5ms | Train: loss=0.230, acc=93.04% | Valid: loss=0.248, acc=92.63% | *\n",
      "| Epoch   2, time=390.9ms/159.1ms | Train: loss=0.143, acc=95.70% | Valid: loss=0.165, acc=95.08% | *\n",
      "| Epoch   3, time=385.5ms/172.8ms | Train: loss=0.110, acc=96.72% | Valid: loss=0.133, acc=96.16% | *\n",
      "| Epoch   4, time=380.4ms/164.6ms | Train: loss=0.080, acc=97.58% | Valid: loss=0.112, acc=96.69% | *\n",
      "| Epoch   5, time=375.0ms/160.1ms | Train: loss=0.060, acc=98.21% | Valid: loss=0.099, acc=97.04% | *\n",
      "| Epoch   6, time=372.6ms/163.5ms | Train: loss=0.049, acc=98.59% | Valid: loss=0.094, acc=97.20% | *\n",
      "| Epoch   7, time=371.3ms/160.1ms | Train: loss=0.039, acc=98.85% | Valid: loss=0.090, acc=97.28% | *\n",
      "| Epoch   8, time=365.9ms/159.5ms | Train: loss=0.043, acc=98.65% | Valid: loss=0.102, acc=97.05% |\n",
      "| Epoch   9, time=365.2ms/157.6ms | Train: loss=0.030, acc=99.11% | Valid: loss=0.094, acc=97.18% |\n",
      "| Epoch  10, time=373.4ms/155.6ms | Train: loss=0.019, acc=99.46% | Valid: loss=0.083, acc=97.66% | *\n",
      "| Epoch  11, time=371.5ms/156.6ms | Train: loss=0.017, acc=99.50% | Valid: loss=0.084, acc=97.59% |\n",
      "| Epoch  12, time=380.6ms/160.6ms | Train: loss=0.013, acc=99.63% | Valid: loss=0.085, acc=97.67% | *\n",
      "| Epoch  13, time=371.7ms/161.6ms | Train: loss=0.015, acc=99.54% | Valid: loss=0.092, acc=97.66% |\n",
      "| Epoch  14, time=373.1ms/164.6ms | Train: loss=0.008, acc=99.81% | Valid: loss=0.087, acc=97.66% |\n",
      "| Epoch  15, time=379.8ms/180.5ms | Train: loss=0.008, acc=99.79% | Valid: loss=0.092, acc=97.70% | *\n",
      "| Epoch  16, time=401.8ms/162.6ms | Train: loss=0.007, acc=99.80% | Valid: loss=0.094, acc=97.74% | *\n",
      "| Epoch  17, time=384.5ms/164.5ms | Train: loss=0.012, acc=99.56% | Valid: loss=0.113, acc=97.42% |\n",
      "| Epoch  18, time=423.9ms/164.0ms | Train: loss=0.010, acc=99.68% | Valid: loss=0.102, acc=97.57% |\n",
      "| Epoch  19, time=370.7ms/167.5ms | Train: loss=0.009, acc=99.71% | Valid: loss=0.099, acc=97.67% |\n",
      "| Epoch  20, time=378.0ms/168.5ms | Train: loss=0.006, acc=99.82% | Valid: loss=0.107, acc=97.81% | *\n",
      "| Epoch  21, time=397.9ms/167.1ms | Train: loss=0.003, acc=99.91% | Valid: loss=0.103, acc=97.96% | *\n",
      "| Epoch  22, time=383.5ms/163.6ms | Train: loss=0.011, acc=99.62% | Valid: loss=0.116, acc=97.64% |\n",
      "| Epoch  23, time=367.0ms/161.1ms | Train: loss=0.010, acc=99.65% | Valid: loss=0.132, acc=97.45% |\n",
      "| Epoch  24, time=367.7ms/170.5ms | Train: loss=0.005, acc=99.82% | Valid: loss=0.120, acc=97.62% |\n",
      "| Epoch  25, time=394.5ms/181.5ms | Train: loss=0.009, acc=99.70% | Valid: loss=0.126, acc=97.45% |\n",
      "| Epoch  26, time=381.3ms/166.6ms | Train: loss=0.011, acc=99.65% | Valid: loss=0.131, acc=97.45% | lr=3.3e-04\n",
      "| Epoch  27, time=368.0ms/165.1ms | Train: loss=0.001, acc=100.00% | Valid: loss=0.110, acc=98.01% | *\n",
      "| Epoch  28, time=370.0ms/172.5ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.112, acc=98.07% | *\n",
      "| Epoch  29, time=374.0ms/174.1ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.113, acc=98.09% | *\n",
      "| Epoch  30, time=381.0ms/163.6ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.117, acc=98.00% |\n",
      "| Epoch  31, time=364.4ms/162.5ms | Train: loss=0.001, acc=99.98% | Valid: loss=0.126, acc=97.81% |\n",
      "| Epoch  32, time=376.0ms/155.6ms | Train: loss=0.001, acc=99.97% | Valid: loss=0.128, acc=97.84% |\n",
      "| Epoch  33, time=362.1ms/163.5ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.120, acc=97.94% |\n",
      "| Epoch  34, time=376.0ms/160.4ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.121, acc=97.96% | lr=1.1e-04\n",
      "| Epoch  35, time=371.2ms/166.6ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.123, acc=98.07% |\n",
      "| Epoch  36, time=368.0ms/170.5ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.126, acc=98.04% |\n",
      "| Epoch  37, time=412.1ms/175.1ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.130, acc=98.01% |\n",
      "| Epoch  38, time=410.2ms/161.5ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.131, acc=98.03% |\n",
      "| Epoch  39, time=387.0ms/165.6ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.132, acc=98.05% | lr=3.7e-05\n",
      "| Epoch  40, time=371.9ms/161.6ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.134, acc=98.02% |\n",
      "| Epoch  41, time=367.4ms/169.5ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.134, acc=98.09% |\n",
      "| Epoch  42, time=364.6ms/179.0ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.135, acc=98.04% |\n",
      "| Epoch  43, time=380.7ms/174.5ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.134, acc=98.08% |\n",
      "| Epoch  44, time=383.9ms/162.6ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.136, acc=98.06% | lr=1.2e-05\n",
      "| Epoch  45, time=385.6ms/168.6ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.136, acc=98.03% |\n",
      "| Epoch  46, time=429.1ms/162.6ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.136, acc=98.07% |\n",
      "| Epoch  47, time=368.3ms/163.6ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.136, acc=98.08% |\n",
      "| Epoch  48, time=417.9ms/166.7ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.137, acc=98.03% |\n",
      "| Epoch  49, time=389.8ms/162.5ms | Train: loss=0.000, acc=100.00% | Valid: loss=0.137, acc=98.03% | lr=4.1e-06\n",
      "test acc: (0.10823734484701454, 0.9802)\n"
     ]
    }
   ],
   "source": [
    "appr.train(x_train, y_train, x_valid, y_valid)\n",
    "print('test acc:', appr.eval(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0], device='cuda:0')\n",
      "tensor(199., device='cuda:0')\n",
      "tensor(14.9500, device='cuda:0', grad_fn=<UnbindBackward>)\n",
      "tensor(184., device='cuda:0')\n",
      "tensor(20.5522, device='cuda:0', grad_fn=<UnbindBackward>)\n",
      "tensor(4., device='cuda:0')\n",
      "tensor(14.5686, device='cuda:0', grad_fn=<UnbindBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softplus\n",
    "import torch.nn.functional as F\n",
    "from networks.mlp import LinearSparse\n",
    "i = [100]\n",
    "x = x_train[i]\n",
    "x = x.view(x.size(0), -1)\n",
    "y = y_train[i]\n",
    "print(y)\n",
    "\n",
    "for m in model.modules():\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "#         print(F.relu(m.mask_layer(x)).shape)\n",
    "        x = F.relu(m(x))\n",
    "        print((x==0).float().sum())\n",
    "        print(max(x[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(122., device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([277.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        123.]),\n",
       " array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "        0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "        0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "        0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "        0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "        0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "        0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "        0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "        0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "        0.99, 1.  ], dtype=float32),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "mask = torch.sigmoid(400*model.mask1).squeeze()\n",
    "print((mask==1).float().sum())\n",
    "plt.hist(mask.cpu().detach().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 784])\n",
      "torch.Size([50000, 400])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "tensor(37., device='cuda:0')\n",
      "tensor(13.9359, device='cuda:0', grad_fn=<UnbindBackward>) tensor(0., device='cuda:0', grad_fn=<UnbindBackward>)\n",
      "tensor(2., device='cuda:0')\n",
      "tensor(54.6328, device='cuda:0', grad_fn=<UnbindBackward>) tensor(0., device='cuda:0', grad_fn=<UnbindBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([140., 121.,  56.,  24.,  16.,   6.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   2.,   1.,   1.,   2.,   1.,   1.,   0.,   0.,\n",
       "          3.,   1.,   0.,   3.,   0.,   0.,   2.,   1.,   1.,   1.,   0.,\n",
       "          1.,   2.,   1.,   0.,   0.,   2.,   0.,   1.,   1.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   2.,   0.,   0.,   0.,   0.,   0.,   1.,\n",
       "          1.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          1.]),\n",
       " array([ 0.        ,  0.1393588 ,  0.2787176 ,  0.4180764 ,  0.5574352 ,\n",
       "         0.696794  ,  0.8361528 ,  0.97551155,  1.1148704 ,  1.2542292 ,\n",
       "         1.393588  ,  1.5329468 ,  1.6723056 ,  1.8116643 ,  1.9510231 ,\n",
       "         2.0903819 ,  2.2297409 ,  2.3690996 ,  2.5084584 ,  2.6478171 ,\n",
       "         2.787176  ,  2.9265347 ,  3.0658937 ,  3.2052524 ,  3.3446112 ,\n",
       "         3.48397   ,  3.6233287 ,  3.7626874 ,  3.9020462 ,  4.041405  ,\n",
       "         4.1807637 ,  4.3201227 ,  4.4594817 ,  4.59884   ,  4.738199  ,\n",
       "         4.8775578 ,  5.0169168 ,  5.1562753 ,  5.2956343 ,  5.4349933 ,\n",
       "         5.574352  ,  5.713711  ,  5.8530693 ,  5.9924283 ,  6.1317873 ,\n",
       "         6.271146  ,  6.410505  ,  6.5498633 ,  6.6892223 ,  6.828581  ,\n",
       "         6.96794   ,  7.107299  ,  7.2466574 ,  7.3860164 ,  7.525375  ,\n",
       "         7.664734  ,  7.8040924 ,  7.9434514 ,  8.08281   ,  8.222169  ,\n",
       "         8.361527  ,  8.500887  ,  8.640245  ,  8.779604  ,  8.918963  ,\n",
       "         9.058322  ,  9.19768   ,  9.337039  ,  9.476398  ,  9.615757  ,\n",
       "         9.7551155 ,  9.894475  , 10.0338335 , 10.173192  , 10.312551  ,\n",
       "        10.45191   , 10.591269  , 10.730627  , 10.869987  , 11.009345  ,\n",
       "        11.148704  , 11.288062  , 11.427422  , 11.56678   , 11.706139  ,\n",
       "        11.845498  , 11.984857  , 12.124215  , 12.263575  , 12.402933  ,\n",
       "        12.542292  , 12.68165   , 12.82101   , 12.960368  , 13.099727  ,\n",
       "        13.239086  , 13.378445  , 13.517803  , 13.657162  , 13.796521  ,\n",
       "        13.93588   ], dtype=float32),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# r = np.arange(x.size(0))\n",
    "# r = torch.LongTensor(r)\n",
    "x = x_train.view(x_train.size(0), -1)\n",
    "print(x.shape)\n",
    "out1 = F.relu(model.fc1(x))\n",
    "out2 = F.relu(model.fc2(out1))\n",
    "out3 = model.fc3(out2)\n",
    "# for i in range(0,len(r),self.sbatch):\n",
    "#     if i+self.sbatch<=len(r): b=r[i:i+self.sbatch]\n",
    "#     else: b=r[i:]\n",
    "#     images=x[b].to(device)\n",
    "#     targets=y[b].to(device)\n",
    "out = out1\n",
    "print(out.shape)\n",
    "var = torch.var(out, dim=0)\n",
    "print(var.shape)\n",
    "# print(var)\n",
    "mean = torch.mean(out, dim=0)\n",
    "print(mean.shape)\n",
    "# print(mean)\n",
    "print((mean > 1).float().sum())\n",
    "print(max(mean), min(mean))\n",
    "print((var==0).float().sum())\n",
    "print(max(var), min(var))\n",
    "plt.hist(mean.cpu().detach().numpy(), bins=100)\n",
    "# plt.hist(var.cpu().detach().numpy(), bins=100)\n",
    "# print(model.fc3(mean))\n",
    "# print(torch.mean(out3, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784])\n"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'Linear' object has no attribute 'rad_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-f30ed56ac54a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# print(softplus(layer.dir_softplus_inv_concentration))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrad_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    777\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 779\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleAttributeError\u001b[0m: 'Linear' object has no attribute 'rad_layer'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softplus\n",
    "import torch.nn.functional as F\n",
    "x = x_train[:1]\n",
    "x = x.view(x.size(0), -1)\n",
    "print(x.shape)\n",
    "layer = model.fc1\n",
    "# print(softplus(layer.dir_softplus_inv_concentration))\n",
    "rad = F.relu(layer.rad_layer(x))\n",
    "print(rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0482],\n",
      "        [0.0391],\n",
      "        [0.0483],\n",
      "        [0.0492],\n",
      "        [0.0503],\n",
      "        [0.0448],\n",
      "        [0.0437],\n",
      "        [0.0474],\n",
      "        [0.0436],\n",
      "        [0.0480],\n",
      "        [0.0502],\n",
      "        [0.0505],\n",
      "        [0.0490],\n",
      "        [0.0498],\n",
      "        [0.0480],\n",
      "        [0.0486],\n",
      "        [0.0463],\n",
      "        [0.0509],\n",
      "        [0.0468],\n",
      "        [0.0465],\n",
      "        [0.0468],\n",
      "        [0.0477],\n",
      "        [0.0499],\n",
      "        [0.0478],\n",
      "        [0.0458],\n",
      "        [0.0507],\n",
      "        [0.0546],\n",
      "        [0.0486],\n",
      "        [0.0454],\n",
      "        [0.0446],\n",
      "        [0.0402],\n",
      "        [0.0589],\n",
      "        [0.0530],\n",
      "        [0.0487],\n",
      "        [0.0410],\n",
      "        [0.0487],\n",
      "        [0.0516],\n",
      "        [0.0519],\n",
      "        [0.0488],\n",
      "        [0.0482],\n",
      "        [0.0459],\n",
      "        [0.0403],\n",
      "        [0.0484],\n",
      "        [0.0495],\n",
      "        [0.0446],\n",
      "        [0.0508],\n",
      "        [0.0390],\n",
      "        [0.0539],\n",
      "        [0.0459],\n",
      "        [0.0491],\n",
      "        [0.0454],\n",
      "        [0.0450],\n",
      "        [0.0398],\n",
      "        [0.0494],\n",
      "        [0.0463],\n",
      "        [0.0447],\n",
      "        [0.0519],\n",
      "        [0.0500],\n",
      "        [0.0498],\n",
      "        [0.0503],\n",
      "        [0.0440],\n",
      "        [0.0452],\n",
      "        [0.0491],\n",
      "        [0.0413],\n",
      "        [0.0508],\n",
      "        [0.0445],\n",
      "        [0.0499],\n",
      "        [0.0506],\n",
      "        [0.0424],\n",
      "        [0.0418],\n",
      "        [0.0504],\n",
      "        [0.0491],\n",
      "        [0.0489],\n",
      "        [0.0468],\n",
      "        [0.0455],\n",
      "        [0.0457],\n",
      "        [0.0559],\n",
      "        [0.0425],\n",
      "        [0.0445],\n",
      "        [0.0481],\n",
      "        [0.0555],\n",
      "        [0.0434],\n",
      "        [0.0495],\n",
      "        [0.0448],\n",
      "        [0.0487],\n",
      "        [0.0533],\n",
      "        [0.0424],\n",
      "        [0.0417],\n",
      "        [0.0491],\n",
      "        [0.0503],\n",
      "        [0.0445],\n",
      "        [0.0593],\n",
      "        [0.0533],\n",
      "        [0.0429],\n",
      "        [0.0428],\n",
      "        [0.0415],\n",
      "        [0.0524],\n",
      "        [0.0545],\n",
      "        [0.0473],\n",
      "        [0.0475],\n",
      "        [0.0439],\n",
      "        [0.0496],\n",
      "        [0.0452],\n",
      "        [0.0435],\n",
      "        [0.0422],\n",
      "        [0.0438],\n",
      "        [0.0437],\n",
      "        [0.0503],\n",
      "        [0.0462],\n",
      "        [0.0515],\n",
      "        [0.0378],\n",
      "        [0.0417],\n",
      "        [0.0441],\n",
      "        [0.0468],\n",
      "        [0.0488],\n",
      "        [0.0408],\n",
      "        [0.0409],\n",
      "        [0.0487],\n",
      "        [0.0456],\n",
      "        [0.0508],\n",
      "        [0.0427],\n",
      "        [0.0414],\n",
      "        [0.0469],\n",
      "        [0.0491],\n",
      "        [0.0460],\n",
      "        [0.0432],\n",
      "        [0.0413],\n",
      "        [0.0494],\n",
      "        [0.0483],\n",
      "        [0.0460],\n",
      "        [0.0470],\n",
      "        [0.0478],\n",
      "        [0.0431],\n",
      "        [0.0429],\n",
      "        [0.0511],\n",
      "        [0.0532],\n",
      "        [0.0520],\n",
      "        [0.0436],\n",
      "        [0.0441],\n",
      "        [0.0511],\n",
      "        [0.0410],\n",
      "        [0.0560],\n",
      "        [0.0516],\n",
      "        [0.0443],\n",
      "        [0.0403],\n",
      "        [0.0497],\n",
      "        [0.0517],\n",
      "        [0.0485],\n",
      "        [0.0489],\n",
      "        [0.0494],\n",
      "        [0.0459],\n",
      "        [0.0358],\n",
      "        [0.0432],\n",
      "        [0.0528],\n",
      "        [0.0522],\n",
      "        [0.0427],\n",
      "        [0.0457],\n",
      "        [0.0475],\n",
      "        [0.0466],\n",
      "        [0.0490],\n",
      "        [0.0534],\n",
      "        [0.0462],\n",
      "        [0.0434],\n",
      "        [0.0538],\n",
      "        [0.0451],\n",
      "        [0.0433],\n",
      "        [0.0447],\n",
      "        [0.0494],\n",
      "        [0.0459],\n",
      "        [0.0477],\n",
      "        [0.0461],\n",
      "        [0.0452],\n",
      "        [0.0566],\n",
      "        [0.0450],\n",
      "        [0.0506],\n",
      "        [0.0482],\n",
      "        [0.0523],\n",
      "        [0.0426],\n",
      "        [0.0511],\n",
      "        [0.0442],\n",
      "        [0.0472],\n",
      "        [0.0455],\n",
      "        [0.0442],\n",
      "        [0.0497],\n",
      "        [0.0479],\n",
      "        [0.0489],\n",
      "        [0.0489],\n",
      "        [0.0472],\n",
      "        [0.0394],\n",
      "        [0.0478],\n",
      "        [0.0463],\n",
      "        [0.0397],\n",
      "        [0.0481],\n",
      "        [0.0418],\n",
      "        [0.0420],\n",
      "        [0.0491],\n",
      "        [0.0450],\n",
      "        [0.0450],\n",
      "        [0.0451],\n",
      "        [0.0489],\n",
      "        [0.0413],\n",
      "        [0.0406],\n",
      "        [0.0443],\n",
      "        [0.0436],\n",
      "        [0.0448],\n",
      "        [0.0407],\n",
      "        [0.0507],\n",
      "        [0.0442],\n",
      "        [0.0452],\n",
      "        [0.0460],\n",
      "        [0.0527],\n",
      "        [0.0516],\n",
      "        [0.0457],\n",
      "        [0.0436],\n",
      "        [0.0463],\n",
      "        [0.0471],\n",
      "        [0.0483],\n",
      "        [0.0450],\n",
      "        [0.0467],\n",
      "        [0.0456],\n",
      "        [0.0475],\n",
      "        [0.0453],\n",
      "        [0.0453],\n",
      "        [0.0408],\n",
      "        [0.0499],\n",
      "        [0.0421],\n",
      "        [0.0518],\n",
      "        [0.0416],\n",
      "        [0.0481],\n",
      "        [0.0499],\n",
      "        [0.0510],\n",
      "        [0.0494],\n",
      "        [0.0430],\n",
      "        [0.0436],\n",
      "        [0.0468],\n",
      "        [0.0426],\n",
      "        [0.0464],\n",
      "        [0.0415],\n",
      "        [0.0487],\n",
      "        [0.0477],\n",
      "        [0.0490],\n",
      "        [0.0494],\n",
      "        [0.0553],\n",
      "        [0.0488],\n",
      "        [0.0540],\n",
      "        [0.0530],\n",
      "        [0.0403],\n",
      "        [0.0491],\n",
      "        [0.0485],\n",
      "        [0.0495],\n",
      "        [0.0451],\n",
      "        [0.0418],\n",
      "        [0.0417],\n",
      "        [0.0494],\n",
      "        [0.0436],\n",
      "        [0.0467],\n",
      "        [0.0475],\n",
      "        [0.0471],\n",
      "        [0.0536],\n",
      "        [0.0404],\n",
      "        [0.0479],\n",
      "        [0.0499],\n",
      "        [0.0441],\n",
      "        [0.0487],\n",
      "        [0.0491],\n",
      "        [0.0474],\n",
      "        [0.0397],\n",
      "        [0.0525],\n",
      "        [0.0497],\n",
      "        [0.0458],\n",
      "        [0.0546],\n",
      "        [0.0550],\n",
      "        [0.0484],\n",
      "        [0.0438],\n",
      "        [0.0501],\n",
      "        [0.0465],\n",
      "        [0.0409],\n",
      "        [0.0433],\n",
      "        [0.0405],\n",
      "        [0.0494],\n",
      "        [0.0454],\n",
      "        [0.0452],\n",
      "        [0.0459],\n",
      "        [0.0440],\n",
      "        [0.0452],\n",
      "        [0.0463],\n",
      "        [0.0491],\n",
      "        [0.0465],\n",
      "        [0.0489],\n",
      "        [0.0385],\n",
      "        [0.0435],\n",
      "        [0.0514],\n",
      "        [0.0482],\n",
      "        [0.0380],\n",
      "        [0.0486],\n",
      "        [0.0486],\n",
      "        [0.0420],\n",
      "        [0.0489],\n",
      "        [0.0437],\n",
      "        [0.0493],\n",
      "        [0.0448],\n",
      "        [0.0497],\n",
      "        [0.0472],\n",
      "        [0.0475],\n",
      "        [0.0474],\n",
      "        [0.0504],\n",
      "        [0.0509],\n",
      "        [0.0459],\n",
      "        [0.0458],\n",
      "        [0.0445],\n",
      "        [0.0388],\n",
      "        [0.0528],\n",
      "        [0.0435],\n",
      "        [0.0479],\n",
      "        [0.0487],\n",
      "        [0.0468],\n",
      "        [0.0462],\n",
      "        [0.0424],\n",
      "        [0.0492],\n",
      "        [0.0477],\n",
      "        [0.0498],\n",
      "        [0.0423],\n",
      "        [0.0498],\n",
      "        [0.0441],\n",
      "        [0.0440],\n",
      "        [0.0479],\n",
      "        [0.0551],\n",
      "        [0.0445],\n",
      "        [0.0483],\n",
      "        [0.0433],\n",
      "        [0.0471],\n",
      "        [0.0414],\n",
      "        [0.0542],\n",
      "        [0.0477],\n",
      "        [0.0477],\n",
      "        [0.0455],\n",
      "        [0.0494],\n",
      "        [0.0486],\n",
      "        [0.0416],\n",
      "        [0.0441],\n",
      "        [0.0442],\n",
      "        [0.0463],\n",
      "        [0.0457],\n",
      "        [0.0511],\n",
      "        [0.0487],\n",
      "        [0.0437],\n",
      "        [0.0455],\n",
      "        [0.0414],\n",
      "        [0.0487],\n",
      "        [0.0567],\n",
      "        [0.0503],\n",
      "        [0.0515],\n",
      "        [0.0492],\n",
      "        [0.0421],\n",
      "        [0.0470],\n",
      "        [0.0487],\n",
      "        [0.0560],\n",
      "        [0.0469],\n",
      "        [0.0507],\n",
      "        [0.0467],\n",
      "        [0.0532],\n",
      "        [0.0443],\n",
      "        [0.0480],\n",
      "        [0.0465],\n",
      "        [0.0464],\n",
      "        [0.0484],\n",
      "        [0.0439],\n",
      "        [0.0495],\n",
      "        [0.0491],\n",
      "        [0.0459],\n",
      "        [0.0501],\n",
      "        [0.0373],\n",
      "        [0.0526],\n",
      "        [0.0447],\n",
      "        [0.0437],\n",
      "        [0.0488],\n",
      "        [0.0491],\n",
      "        [0.0414],\n",
      "        [0.0490],\n",
      "        [0.0464],\n",
      "        [0.0443],\n",
      "        [0.0417],\n",
      "        [0.0495],\n",
      "        [0.0426],\n",
      "        [0.0467],\n",
      "        [0.0393],\n",
      "        [0.0457],\n",
      "        [0.0467],\n",
      "        [0.0419],\n",
      "        [0.0515],\n",
      "        [0.0454],\n",
      "        [0.0413],\n",
      "        [0.0464],\n",
      "        [0.0458],\n",
      "        [0.0459],\n",
      "        [0.0451],\n",
      "        [0.0441],\n",
      "        [0.0423],\n",
      "        [0.0497],\n",
      "        [0.0526]], device='cuda:0', grad_fn=<SoftplusBackward>)\n",
      "tensor([1880.5166, 1880.5233, 1880.5472, 1880.5264, 1880.5242, 1880.5427,\n",
      "        1880.5348, 1880.5232, 1880.5295, 1880.5192, 1880.5360, 1880.5151,\n",
      "        1880.5264, 1880.5223, 1880.5217, 1880.5264, 1880.5251, 1880.5264,\n",
      "        1880.5181, 1880.5194, 1880.5260, 1880.5264, 1880.5264, 1880.5480,\n",
      "        1880.5264, 1880.5199, 1880.5300, 1880.5264, 1880.5211, 1880.5254,\n",
      "        1880.5216, 1880.5264, 1880.5264, 1880.5264, 1880.5234, 1880.5264,\n",
      "        1880.5229, 1880.5264, 1880.5264, 1880.5262, 1880.5260, 1880.5227,\n",
      "        1880.5264, 1880.5264, 1880.5172, 1880.5264, 1880.5306, 1880.5264,\n",
      "        1880.5210, 1880.5264, 1880.5201, 1880.5208, 1880.5234, 1880.5264,\n",
      "        1880.5264, 1880.5219, 1880.5233, 1880.5264, 1880.5264, 1880.5264,\n",
      "        1880.5504, 1880.5381, 1880.5238, 1880.5454, 1880.5264, 1880.5359,\n",
      "        1880.5264, 1880.5264, 1880.5266, 1880.5162, 1880.5245, 1880.5186,\n",
      "        1880.5264, 1880.5476, 1880.5161, 1880.5199, 1880.5264, 1880.5529,\n",
      "        1880.5215, 1880.5251, 1880.5264, 1880.5155, 1880.5264, 1880.5297,\n",
      "        1880.5264, 1880.5264, 1880.5225, 1880.5205, 1880.5251, 1880.5244,\n",
      "        1880.5288, 1880.5264, 1880.5264, 1880.5186, 1880.5111, 1880.5225,\n",
      "        1880.5264, 1880.5264, 1880.5217, 1880.5264, 1880.5247, 1880.5264,\n",
      "        1880.5167, 1880.5256, 1880.5162, 1880.5256, 1880.5262, 1880.5264,\n",
      "        1880.5216, 1880.5264, 1880.5481, 1880.5192, 1880.5337, 1880.5250,\n",
      "        1880.5264, 1880.5243, 1880.5480, 1880.5546, 1880.5298, 1880.5264,\n",
      "        1880.5411, 1880.5656, 1880.5323, 1880.5264, 1880.5215, 1880.5370,\n",
      "        1880.5350, 1880.5264, 1880.5176, 1880.5292, 1880.5323, 1880.5227,\n",
      "        1880.5179, 1880.5226, 1880.5232, 1880.5248, 1880.5264, 1880.5229,\n",
      "        1880.5233, 1880.5205, 1880.5436, 1880.5175, 1880.5264, 1880.5450,\n",
      "        1880.5184, 1880.5214, 1880.5264, 1880.5264, 1880.5264, 1880.5264,\n",
      "        1880.5137, 1880.5596, 1880.5393, 1880.5212, 1880.5228, 1880.5260,\n",
      "        1880.5209, 1880.5242, 1880.5264, 1880.5253, 1880.5248, 1880.5172,\n",
      "        1880.5425, 1880.5228, 1880.5255, 1880.5228, 1880.5173, 1880.5264,\n",
      "        1880.5383, 1880.5306, 1880.5167, 1880.5262, 1880.5206, 1880.5452,\n",
      "        1880.5264, 1880.5448, 1880.5264, 1880.5304, 1880.5255, 1880.5222,\n",
      "        1880.5200, 1880.5200, 1880.5186, 1880.5525, 1880.5330, 1880.5156,\n",
      "        1880.5264, 1880.5204, 1880.5269, 1880.5444, 1880.5464, 1880.5221,\n",
      "        1880.5264, 1880.5171, 1880.5233, 1880.5264, 1880.5223, 1880.5499,\n",
      "        1880.5457, 1880.5210, 1880.5266, 1880.5386, 1880.5211, 1880.5149,\n",
      "        1880.5210, 1880.5360, 1880.5264, 1880.5264, 1880.5280, 1880.5217,\n",
      "        1880.5248, 1880.5352, 1880.5248, 1880.5210, 1880.5348, 1880.5131,\n",
      "        1880.5208, 1880.5474, 1880.5242, 1880.5204, 1880.5399, 1880.5193,\n",
      "        1880.5563, 1880.5286, 1880.5411, 1880.5375, 1880.5200, 1880.5288,\n",
      "        1880.5205, 1880.5217, 1880.5264, 1880.5264, 1880.5204, 1880.5255,\n",
      "        1880.5254, 1880.5229, 1880.5237, 1880.5184, 1880.5264, 1880.5264,\n",
      "        1880.5264, 1880.5264, 1880.5264, 1880.5264, 1880.5179, 1880.5264,\n",
      "        1880.5669, 1880.5264, 1880.5264, 1880.5233, 1880.5348, 1880.5314,\n",
      "        1880.5408, 1880.5243, 1880.5245, 1880.5264, 1880.5160, 1880.5399,\n",
      "        1880.5264, 1880.5255, 1880.5308, 1880.5264, 1880.5360, 1880.5264,\n",
      "        1880.5264, 1880.5222, 1880.5261, 1880.5240, 1880.5264, 1880.5251,\n",
      "        1880.5264, 1880.5259, 1880.5264, 1880.5222, 1880.5264, 1880.5248,\n",
      "        1880.5465, 1880.5400, 1880.5177, 1880.5264, 1880.5466, 1880.5239,\n",
      "        1880.5404, 1880.5264, 1880.5240, 1880.5188, 1880.5546, 1880.5288,\n",
      "        1880.5264, 1880.5503, 1880.5206, 1880.5219, 1880.5260, 1880.5371,\n",
      "        1880.5215, 1880.5210, 1880.5397, 1880.5198, 1880.5276, 1880.5264,\n",
      "        1880.5326, 1880.5254, 1880.5272, 1880.5349, 1880.5406, 1880.5193,\n",
      "        1880.5264, 1880.5236, 1880.5264, 1880.5328, 1880.5292, 1880.5190,\n",
      "        1880.5656, 1880.5321, 1880.5231, 1880.5264, 1880.5253, 1880.5353,\n",
      "        1880.5244, 1880.5238, 1880.5264, 1880.5315, 1880.5264, 1880.5341,\n",
      "        1880.5247, 1880.5166, 1880.5214, 1880.5350, 1880.5159, 1880.5298,\n",
      "        1880.5223, 1880.5187, 1880.5229, 1880.5227, 1880.5205, 1880.5272,\n",
      "        1880.5239, 1880.5215, 1880.5265, 1880.5400, 1880.5253, 1880.5222,\n",
      "        1880.5278, 1880.5264, 1880.5264, 1880.5326, 1880.5277, 1880.5298,\n",
      "        1880.5264, 1880.5264, 1880.5264, 1880.5264, 1880.5264, 1880.5245,\n",
      "        1880.5459, 1880.5262, 1880.5164, 1880.5316, 1880.5247, 1880.5399,\n",
      "        1880.5264, 1880.5344, 1880.5310, 1880.5269, 1880.5250, 1880.5264,\n",
      "        1880.5480, 1880.5264, 1880.5358, 1880.5269, 1880.5264, 1880.5217,\n",
      "        1880.5265, 1880.5334, 1880.5229, 1880.5264, 1880.5251, 1880.5269,\n",
      "        1880.5264, 1880.5184, 1880.5558, 1880.5234, 1880.5264, 1880.5389,\n",
      "        1880.5170, 1880.5178, 1880.5363, 1880.5286, 1880.5216, 1880.5264,\n",
      "        1880.5264, 1880.5350, 1880.5348, 1880.5208, 1880.5321, 1880.5234,\n",
      "        1880.5363, 1880.5565, 1880.5264, 1880.5266], device='cuda:0',\n",
      "       grad_fn=<SoftplusBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def ml_kappa(dim, eps):\n",
    "    return (max(4, dim) - 3.0) / (1.0 - (1.0 - eps) ** 2) * (1.0 - eps)\n",
    "\n",
    "# print(ml_kappa(400, 0.1))\n",
    "layer = model.fc2\n",
    "rho = layer.rad_rho\n",
    "con = layer.dir_softplus_inv_concentration\n",
    "print(F.softplus(rho))\n",
    "print(F.softplus(con))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Net().cuda()\n",
    "model.train()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "x_train = data[0]['train']['x'][:10].cuda()\n",
    "y_train = data[0]['train']['y'][:10].cuda()\n",
    "pred = model(x_train)\n",
    "optim.zero_grad()\n",
    "loss = ce(pred, y_train)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "for n, p in model.named_parameters():\n",
    "    print(n, p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "imshow(data[9]['train']['x'][])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8920, 0.9069, 0.9083, 0.9307, 0.9245, 0.9398, 0.9151, 0.9204, 0.9212,\n",
      "        0.9301], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0544, -0.0115, -0.0181,  ..., -0.0513,  0.0312, -0.0156],\n",
      "        [ 0.0057, -0.0028, -0.0063,  ..., -0.0583, -0.0008,  0.0895],\n",
      "        [-0.0698, -0.0424,  0.0063,  ..., -0.0328, -0.0150,  0.0976],\n",
      "        ...,\n",
      "        [ 0.0920, -0.0196, -0.0246,  ..., -0.0696, -0.0038, -0.0253],\n",
      "        [-0.0764,  0.0008, -0.0113,  ...,  0.1081, -0.0230, -0.0159],\n",
      "        [ 0.0946,  0.0016, -0.0639,  ...,  0.0980, -0.0085,  0.0063]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "tensor([1880.9163, 1880.8918, 1881.0278, 1880.9973, 1880.9521, 1881.0879,\n",
      "        1880.9596, 1881.0292, 1881.0919, 1881.0527], device='cuda:0')\n",
      "tensor([1881.0145, 1881.0060, 1881.1832, 1881.1527, 1881.0667, 1881.2095,\n",
      "        1881.0851, 1881.1855, 1881.2629, 1881.2129], device='cuda:0',\n",
      "       grad_fn=<SoftplusBackward>)\n",
      "tensor([-172.2792, -172.2776, -172.2786, -172.2802, -172.2778, -172.2800,\n",
      "        -172.2785, -172.2797, -172.2805, -172.2814], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "tensor([82.7827, 78.0416, 63.1277, 58.8257, 65.2339, 63.8212, 84.7382, 70.9766,\n",
      "        85.9716, 80.9957], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math \n",
    "from networks.kl_divergence import KL_vMF_kappa_full, KL_Powerspherical\n",
    "import torch.nn.functional as F\n",
    "from networks.power_spherical import PowerSpherical\n",
    "# model = Model(ratio=0.5, eps=0.1).cuda()\n",
    "# appr = Appr(model, model_name='mlp', data_name='pMNIST', lr=0.001, sbatch=256, optim='Adam')\n",
    "# appr.model.load_state_dict(torch.load('./trained_model/{}_task_{}.model'.format(appr.file_name, 0)))\n",
    "# appr.model_old = deepcopy(appr.get_model(appr.model))\n",
    "# appr.saved = 1\n",
    "loc = model.fc3.dir_loc\n",
    "prev_loc = appr.model_old['fc3']['dir_loc']\n",
    "scale = F.softplus(model.fc3.dir_softplus_inv_concentration)\n",
    "prev_scale = F.softplus(appr.model_old['fc3']['dir_softplus_inv_concentration'])\n",
    "# print((loc*loc).sum(-1))\n",
    "# print((prev_loc*prev_loc).sum(-1))\n",
    "\n",
    "print((prev_loc*loc).sum(-1))\n",
    "print(prev_loc.norm(2, dim=-1))\n",
    "print(loc)\n",
    "print(prev_scale)\n",
    "print(scale)\n",
    "q = PowerSpherical(loc, scale)\n",
    "p = PowerSpherical(prev_loc, prev_scale)\n",
    "alpha = q.base_dist.marginal_t.base_dist.concentration1\n",
    "beta = q.base_dist.marginal_t.base_dist.concentration0\n",
    "# print(alpha)\n",
    "# print(beta)\n",
    "# print(torch.lgamma(alpha))\n",
    "# print(torch.lgamma(alpha + beta))\n",
    "# sample = q.rsample()\n",
    "# print(sample)\n",
    "# print((math.log(2) + torch.digamma(alpha) - torch.digamma(alpha + beta)))\n",
    "# print(torch.log1p((loc * loc).sum(-1)))\n",
    "# print(q.log_normalizer())\n",
    "# print(torch.lgamma(alpha) - torch.lgamma(alpha + beta))\n",
    "# print(q.log_prob(sample))\n",
    "print(-q.entropy()-q.log_prob(loc))\n",
    "print(KL_Powerspherical(q, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n_bins = 20\n",
    "\n",
    "# Generate a normal distribution, center at x=0 and y=5\n",
    "a = Normal(0, 1)\n",
    "b = Normal(2, 2)\n",
    "\n",
    "x = a.sample((100000,)) + b.sample((100000, ))*2\n",
    "y = b.sample((100000, ))\n",
    "x = x.numpy()\n",
    "y = y.numpy()\n",
    "# fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "\n",
    "# We can set the number of bins with the `bins` kwarg\n",
    "plt.hist(x, bins=100)\n",
    "plt.hist(y, bins=100)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
